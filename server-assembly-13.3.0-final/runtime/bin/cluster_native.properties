# You can use environment variables like $DQC_HOME/lib/* or ${DQC_HOME}/lib/*

# Sets the HADOOP_HOME environment variable. The property should point to the <DQC_HOME> folder. On a Windows machine, 
# 	the path must contain <DQC_HOME>/bin/winutils.exe.
# If HADOOP_HOME is already set, hadoop.home.dir is optional. If used, it overwrites the original HADOOP_HOME value.
hadoop.home.dir=..



#------------------------------------- LIBRARIES CLUSTER ---------------------------------------------------------
# Classpath containing all required Java libraries. It must contain the <DQC_HOME>/lib/* folder.
# The files in the classpath are copied to the cluster.
# You can set additional classpaths in a separate cp.<custom> property, 
# e.g., cp.drivers for database drivers.
cp.runtime=../lib/*:!scala*.jar:!slf4j-api-*.jar:!kryo-*.jar:!commons-lang3*.jar


#------------------------------------- LIBRARIES LOCAL ----------------------------------------------------------
# lcp - Local Classpath to cluster-version-specific JAR files. These local files are not copied to the cluster.
# You can set additional classpaths in a separate lcp.<custom> property.
# Set lcp.dist to specific Hadoop distribution jar files
# You can use the same !<file_mask> notation as in cp classpath to exclude specific files.
lcp.dist=../lib/ext/*

# Set lcp.spark to the spark assembly files that will be used for starting Spark processing.
lcp.spark=/PATH_TO_SPARK_JARS/*



#---------------------------- AUTHENTICATION and CLIENT CONFIGURATION --------------------------------------------------
# Path to the folder containing all Hadoop client configuration files (e.g., core-site.xml, hdfs-site.xml) for the given cluster.
# Should be specified without asterisk at the end.
conf=/PATH_TO_CLIENT_CONF/

# Cluster authentication settings. Can be either basic or kerberos.
#auth=basic
auth=kerberos

# For a basic authentication, provide the following settings:
# basic.user - the user to be used for processing on cluster.
#basic.user=bde.user

# If the cluster is secured by Kerberos, provide the additional settings:
# Kerberos configuration, typically called krb5.conf
# Kerberos principal to be used for authentication by this BDE.
# Keytab for the principal above. To use a Kerberos ticket instead of the keytab, comment out the kerberos.keytab property. 
kerberos.conf=/opt/ataccama/kerberosConfig/krb5.conf
kerberos.principal=bde.user@DOMAIN.COM
kerberos.keytab=/opt/ataccama/keytabs/bdeuser.keytab



# -------------------------------- MAPREDUCE CONFIGURATION ------------------------------------------------------
# It is possible to override hadoop client configuration files used by BDE via this notation:
#conf.mapreduce.reduce.java.opts=-Xmx2457m
#conf.mapreduce.reduce.memory.mb=3072
#conf.mapreduce.job.reduces=16
#conf.mapreduce.job.jvm.numtasks=4



# -------------------------------- SPARK CONFIGURATION ----------------------------------------------------------
# All properties starting with spark. are passed to Spark. 
# These properties apply to Spark processings only and are ignored in non-Spark processings.
# The required properties are:
# spark.master - must be set to yarn-cluster; other modes are not supported
# spark.io.compression.codec - must be set to lz4
# 
# You can set additional spark.<custom> properties here. 
# You can also use spark.properties to keep spark configuration in a separate file.
# In case a property is set in both cluster.properties and spark.properties, the definition in the cluster.properties takes priority.

#spark.properties=cluster_spark.properties

spark.io.compression.codec=lz4
spark.master=yarn-cluster
spark.sql.catalogImplementation=hive