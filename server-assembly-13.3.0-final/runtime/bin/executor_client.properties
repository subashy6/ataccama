# You can use environment variables like $DQC_HOME/lib/* or ${DQC_HOME}/lib/*

#================================== EXECUTOR CONFIGURATION ============================================================
# Ataccama Executor URL address. 
# Specify the URL depending on your BDE architecture (direct or via Knox).
gateway=http://<your_executor>:8888/executor
#gateway=https://<knox_server>:8443/gateway/default/bde/executor

# Executor timeout in ms
timeout=5000

#------------------------------------ AUTHENTICATION ------------------------------------------------------------
# Executor authentication settings.
gateway.authType=basic

# basic.user - the user to be used for processing on cluster.
basic.user=ataccama

# Password is required when Executor is secured via Knox or Apache Keycloak
#basic.password=<password>


#===================================== CLASSPATH ==================================================================

#------------------------------------- LOCAL CLASSPATH ----------------------------------------------------------
# lcp - Local Classpath to cluster-version-specific JAR files. It must contain the <DQC_HOME>/lib/* folder.
# These local files are not copied to the cluster.
# You can set additional classpaths in a separate lcp.<custom> property.
# lcp.dist should point to specific Hadoop distribution jar files (lib/ext folder by default)
# You can use the same !<file_mask> notation as in cp classpath to exclude specific files.
lcp.runtime=../lib/*
#lcp.dist=../lib/ext/*

#------------------------------------- EXECUTOR CLASSPATH ---------------------------------------------------------
# Executor classpath containing all additional Java libraries that you would like to copy together with your plan
# If the plan is started in Spark or MapReduce mode, the classpath will be copied to the cluster.
# You can set additional classpaths in a separate cp.<custom> property, 
# e.g., cp.drivers for database drivers.
#cp.drivers=../lib/jdbc/mysql/*


#================================== HADOOP CONFIGURATION =======================================================

#--------------------------------- SPARK CONFIGURATION -----------------------------------------------------------
# All properties starting with spark. are passed to Spark. 
# These properties apply to Spark processings only and are ignored in non-Spark processings.
# 
# You can set additional spark.<custom> properties here. 
# You can also use spark.properties to keep spark configuration in a separate file.
# In case a property is set in both cluster.properties and spark.properties, the definition in the cluster.properties takes priority.

#spark.properties=cluster_spark.properties
#spark.yarn.queue=default

# -------------------------------- MAPREDUCE CONFIGURATION ------------------------------------------------------
# It is possible to override Remote Executor default settings used by BDE via this notation:
#conf.mapreduce.reduce.java.opts=-Xmx2457m
#conf.mapreduce.reduce.memory.mb=3072
#conf.mapreduce.job.reduces=16
#conf.mapreduce.job.jvm.numtasks=4
