#------------------------------------------- AUTHENTICATION --------------------------------------------------

# Cluster authentication settings. Can be either basic or kerberos.
#auth=basic
auth=kerberos

# For a basic authentication, provide the following settings:
# basic.user - the user to be used for processing on cluster.
#basic.user=bde.user

# If the cluster is secured by Kerberos, provide the additional settings:
# Kerberos configuration, typically called krb5.conf
# Kerberos principal to be used for authentication by this BDE.
# Keytab for the principal above. To use a Kerberos ticket instead of the keytab, comment out the kerberos.keytab property.
kerberos.conf=/etc/krb5.conf
kerberos.principal=executor@EXAMPLE.COM
kerberos.keytab=keytab/executor.keytab

# Impersonation property allows Executor to start processes on cluster via another user. If none specified, default true
impersonate=false

#----------------------------------- ENVIRONMENT SPARK VARIABLES ---------------------------------------------------

# Set Java if needed.
#env.JAVA_HOME=/usr/java/jdk1.8.0_65

# It is possible to modify Spark driver options via the following variable
env.SPARK_DRIVER_OPTS=--driver-memory 2g












#============================================EXPERT SETTINGS=========================================================
#--------------------------------------- MAPREDUCE CONFIGURATION ------------------------------------------------
# It is possible to override hadoop client configuration files used by BDE via this notation:
#conf.mapreduce.reduce.java.opts=-Xmx2457m
#conf.mapreduce.reduce.memory.mb=3072
#conf.mapreduce.job.reduces=16
#conf.mapreduce.job.jvm.numtasks=4


#conf.hdp.version=<hdp.version>

#------------------------------------ TEMPORARY FILES LOCATION ------------------------------------------------
# Default folders for distributed local files:
# on HDFS used for user files
fsc.shared=/tmp/ataccama_executor_shared_fs/
# on HDFS used for distributing classpath (cp)
fsc.shrcp=/tmp/ataccama_executor_shared_cp/
# on local FS of Hadoop worker nodes
fsc.node=/tmp/ataccama_executor_node/
# Forces to copy files ignoring last modification time
#fsc.force=true

# Default permissions for HDFS
#conf.fs.permissions.umask-mode=022


#------------------------------------- LIBRARIES CLUSTER ---------------------------------------------------------

# Classpath containing all required Java libraries. It must contain the <DQC_HOME>/lib/* folder.
# The files in the classpath are copied to the cluster.
# You can set additional classpaths in a separate cp.<custom> property,
# e.g., cp.drivers for database drivers.

# To exclude specific files from the classpath, you can use the !<file_mask> notation following your classpath:

cp.!exclude=!atc-hive-jdbc*!hive-jdbc*:!slf4j-api-*.jar:!kryo-*.jar:!scala*.jar:!commons-lang3*.jar!cif-dtdb*.jar!janino*

#------------------------------------- MISCELLANEOUS ------------------------------------------------------------
# debug mode. Shows driver and executor classloaders in spark mode
#debug=true


# Path to the folder containing all Hadoop client configuration files (e.g., core-site.xml, hdfs-site.xml) for the given cluster.
# Not needed by default. This configuration is taken automatically from $MYDIR/client_conf/ directory
#conf=/etc/hadoop/conf/

# BDE will attempt to find SPARK_HOME in default locations. If none found, please specify a path to SPARK_HOME manually
#env.SPARK_HOME=<path_to_spark_home> 	
#env.SPARK_HOME=/opt/cloudera/parcels/SPARK2/lib/spark2/
#env.SPARK_HOME=/usr/hdp/current/spark2-client/

# Needed for spark2 distribution without hadoop libs
# Mostly required by CDH
#env.ENABLE_SPARK_DIST_CLASSPATH=true

# Executor will attempt to validate spark properties file
# It is possible to turn off the validation with the following option:
#env.DISABLE_VALIDATE_SPARK_PROPERTIES=true

# Enable debug level of logging in log4j
#env.ENABLE_DEBUG_LOGGING=true

# Provide client configuration to Spark job via Spark variables
#env.ENABLE_SPARK_CONF_DIR=true

# Name of spark-submit script
#env.SPARK_SUBMIT=spark-submit

# Disables timeline service
conf.yarn.timeline-service.enabled=false

exec.SPARK=exec_spark2.sh
exec.MAP_REDUCE=exec_mapreduce.sh
spark.properties=spark.properties