# Properties file used for Ataccama Remote Executor Component

#------------------------------------- LIBRARIES ----------------------------------------------------------

# cp - classpath that will be used for starting Ataccama jobs. It must contain the <DQC_HOME>/lib/* folder.
# The files in the classpath are used for local classpath and and copied to cluster for Spark or MapReduce jobs.
# It is also possible to use "lcp" notations which stands for "local classpath". The files will be used as local classpath only.
# You can set additional classpaths in a separate cp.<custom> property,
# e.g., cp.drivers for database drivers if they are located in a separate folder.
cpdelim=:
cp.!excludeFromRuntime=${EXCLUDE_FROM_RUNTIME}
cp.ovr=../../lib/ovr/*.jar
cp.runtime=../../lib/*.jar
# for server_run and databricks, append ext libraries, the USE_LIB_EXT variable is set by execscript
lcp.runtime_ext=${USE_LIB_EXT+../../lib/ext/*.jar}


# If a license is not located in one of default locations (home folder or {runtime}/license_keys/ folder)
# it is possible to specify another folder
dqc.licenses=../../license_keys/

#--------------------------------- BASIC SETTINGS ----------------------------------------------------------------

# Local file system folder where possibly store outputs of remote processing to.
jobs.directOutputRoot=/tmp/output

#------------------------------------- EXECUTOR SCRIPTS ----------------------------------------------------------
# scripts for submitting runner class in several execution modes
#exec= default if not specified exact type
exec.SERVER_RUN=exec_local.sh
exec.LOCAL=exec_local.sh

# spark run in client or cluster deploy mode
#exec.SPARK=exec_spark2.sh
#exec.SPARK=exec_spark2c.sh


#------------------------------------- CLUSTER CONFIGURATION ----------------------------------------------------------
# Enable this configuration if you are connecting to Hadoop cluster and configure the file accordingly
#hadoop.properties=hadoop.properties

# Enable this configuration if you are connecting to Databricks cluster and configure the file accordingly
#databricks.properties=databricks.properties
