#--------------------------------------- BASIC SETTINGS ----------------------------------------------------------
# Setup launch script for spark (specify .sh script for Linux distribution, .bat script for Windows distribution)
plugin.executor-launch-model.ataccama.one.launch-type-properties.SPARK.exec=${ataccama.path.root}/bin/databricks/exec_databricks.sh

# Setup path to DQC licence if the licence is not present in home folder of the user or in runtime/license_keys
#plugin.executor-launch-model.ataccama.one.launch-type-properties.SPARK.dqc.licenses=../../../lib/runtime/license_keys

# Set Java if needed. The JDK version 8 is REQUIRED to run spark jobs.
#plugin.executor-launch-model.ataccama.one.launch-type-properties.SPARK.env.JAVA_HOME=/usr/java/jdk1.8.0_65

#--------------------------------------- JOB-SPECIFIC SETTINGS ---------------------------------------------------

# Comma-separated list of keys identifying the system properties which are allowed to be set through submitted jobs,
# thus making their values specific for the particular job.
# The job-specific system properties are passed to each spawned DQC runtime JVM associated with the identified launch type (LOCAL, SPARK, ...).
#plugin.executor-launch-model.ataccama.one.launch-type-properties.SPARK.job-specific-system-properties.allowed-keys=\
#  executorProp.spark.ata.readOnlyLatestPartition,\
#  executorProp.spark.ata.enableCustomSqlTemplates

#------------------------------------- LIBRARIES CLUSTER ---------------------------------------------------------

# Classpath containing all required Java libraries. It must contain the <DQC_HOME>/lib/* folder.
# The files in the classpath are copied to the cluster.
# You can set additional classpaths in a separate cp.<custom> property,
# e.g., cp.drivers for database drivers.

# To exclude specific files from the classpath, you can use the !<file_mask> notation following your classpath:

# Relative to temp/jobs/${jobId} from root dir of the repository
plugin.executor-launch-model.ataccama.one.launch-type-properties.SPARK.cpdelim=;
plugin.executor-launch-model.ataccama.one.launch-type-properties.SPARK.cp.runtime=../../../lib/runtime/*;../../../lib/jdbc/*;../../../lib/jdbc_ext/*
plugin.executor-launch-model.ataccama.one.launch-type-properties.SPARK.cp.ovr=../../../lib/ovr/*.jar
plugin.executor-launch-model.ataccama.one.launch-type-properties.SPARK.cp.databricks=../../../lib/runtime/databricks/*
plugin.executor-launch-model.ataccama.one.launch-type-properties.SPARK.cp.!exclude=!atc-hive-jdbc*!hive-jdbc*;!slf4j-api-*.jar;!kryo-*.jar;!scala*.jar;!commons-lang3*.jar!cif-dtdb*.jar
plugin.executor-launch-model.ataccama.one.launch-type-properties.SPARK.lcp.!exclude=!guava-11*
plugin.executor-launch-model.ataccama.one.launch-type-properties.SPARK.lcp.ext=../../../lib/ext/*

#----------------------- GENERAL ----------------------------------------
# debug mode. Shows driver and executor classloaders in spark mode
#plugin.executor-launch-model.ataccama.one.launch-type-properties.SPARK.debug=true

# ID or name of existing cluster, optional - the "youngest" cluster will be assumed when not specified
plugin.executor-launch-model.ataccama.one.launch-type-properties.SPARK.dbr.cluster=ataccama

# test the cluster has right mount/libraries (default true)
# use false for speed up job launching when libraries of cluster is already installed and runtime is unchanged (checking will be skipped)
#plugin.executor-launch-model.ataccama.one.launch-type-properties.SPARK.dbr.checkCluster=false

# Number of concurent job runs
plugin.executor-launch-model.ataccama.one.launch-type-properties.SPARK.dbr.job.max_concurrent_runs=150

# Forces to copy files ignoring last modification time
#plugin.executor-launch-model.ataccama.one.launch-type-properties.SPARK.fsc.force=true

#---------------------------------Authentication-----------------------------------------
# Access to the databricks platform
#
plugin.executor-launch-model.ataccama.one.launch-type-properties.SPARK.dbr.url=https://<YOUR_DATABRICKS_CLUSTER>
plugin.executor-launch-model.ataccama.one.launch-type-properties.SPARK.dbr.authType=PERSONAL_TOKEN
plugin.executor-launch-model.ataccama.one.launch-type-properties.SPARK.dbr.token=<YOUR_TOKEN>

# You can also use username & password authentication instead of token
#plugin.executor-launch-model.ataccama.one.launch-type-properties.SPARK.dbr.user=
#plugin.executor-launch-model.ataccama.one.launch-type-properties.SPARK.dbr.password=



#---------------------------Amazon Web Services---------------------------------------------
# Enable all of these properties and configure authentication if you are using Databricks on AWS
# The same credentials can be used for mounting filesystem and for Ataccama jobs.

# S3 Folder for storing libraries and files
# mounted to some dir in dbfs, e.g.:
#plugin.executor-launch-model.ataccama.one.launch-type-properties.SPARK.mount.point=/mnt/ataccama
#plugin.executor-launch-model.ataccama.one.launch-type-properties.SPARK.mount.url=s3a://.../tmp/dbr

# User Account to be used for mounting filesystem
#plugin.executor-launch-model.ataccama.one.launch-type-properties.SPARK.mount.conf.fs.s3n.awsAccessKeyId=
#plugin.executor-launch-model.ataccama.one.launch-type-properties.SPARK.mount.conf.fs.s3n.awsSecretAccessKey=

# User Account to be used for configuring Ataccama job
#plugin.executor-launch-model.ataccama.one.launch-type-properties.SPARK.conf.fs.s3a.access.key=
#plugin.executor-launch-model.ataccama.one.launch-type-properties.SPARK.conf.fs.s3a.secret.key=

# Uncomment these options if you are using AWS
#plugin.executor-launch-model.ataccama.one.launch-type-properties.SPARK.conf.fs.s3a.fast.upload=true
#plugin.executor-launch-model.ataccama.one.launch-type-properties.SPARK.conf.fs.s3a.fast.upload.buffer=bytebuffer


#---------------------------------Azure--------------------------------------------------
# Enable all of these properties and configure authentication if you are using Databricks on Azure
# The same credentials can be used for mounting filesystem and for Ataccama jobs.

# ADLS folder for storing libraries and files
# mounted to some dir in dbfs, e.g.:
#plugin.executor-launch-model.ataccama.one.launch-type-properties.SPARK.mount.point=/mnt/ataccama
#plugin.executor-launch-model.ataccama.one.launch-type-properties.SPARK.mount.url=adl://.../tmp/dbr


#plugin.executor-launch-model.ataccama.one.launch-type-properties.SPARK.mount.conf.dfs.adls.oauth2.access.token.provider.type=ClientCredential
#plugin.executor-launch-model.ataccama.one.launch-type-properties.SPARK.mount.conf.dfs.adls.oauth2.refresh.url=<https://login.microsoftonline.com/.../oauth2/token>
#plugin.executor-launch-model.ataccama.one.launch-type-properties.SPARK.mount.conf.dfs.adls.oauth2.client.id=
#plugin.executor-launch-model.ataccama.one.launch-type-properties.SPARK.mount.conf.dfs.adls.oauth2.credential=

#plugin.executor-launch-model.ataccama.one.launch-type-properties.SPARK.conf.fs.adl.impl=org.apache.hadoop.fs.adl.AdlFileSystem

#plugin.executor-launch-model.ataccama.one.launch-type-properties.SPARK.conf.dfs.adls.oauth2.access.token.provider.type=ClientCredential
#plugin.executor-launch-model.ataccama.one.launch-type-properties.SPARK.conf.dfs.adls.oauth2.refresh.url=<https://login.microsoftonline.com/.../oauth2/token>
#plugin.executor-launch-model.ataccama.one.launch-type-properties.SPARK.conf.dfs.adls.oauth2.client.id=
#plugin.executor-launch-model.ataccama.one.launch-type-properties.SPARK.conf.dfs.adls.oauth2.credential=
