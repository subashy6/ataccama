# Note: configuring Spark via Hadoop is currently supported only for linux platform.

#--------------------------------------- BASIC SETTINGS ----------------------------------------------------------
# Setup launch script for spark
plugin.executor-launch-model.ataccama.one.launch-type-properties.SPARK.exec=${ataccama.path.root}/bin/hadoop/exec_spark2.sh

# Setup path to DQC licence if the licence is not present in home folder of the user or in runtime/license_keys
#plugin.executor-launch-model.ataccama.one.launch-type-properties.SPARK.dqc.licenses=../../../lib/runtime/license_keys

# Set Java if needed. The JDK version 8 is REQUIRED to run spark jobs.
#plugin.executor-launch-model.ataccama.one.launch-type-properties.SPARK.env.JAVA_HOME=/usr/java/jdk1.8.0_65

# It is possible to modify Spark driver options via the following variable
plugin.executor-launch-model.ataccama.one.launch-type-properties.SPARK.env.SPARK_DRIVER_OPTS=--driver-memory 2g


#--------------------------------------- JOB-SPECIFIC SETTINGS ---------------------------------------------------

# Comma-separated list of keys identifying the system properties which are allowed to be set through submitted jobs,
# thus making their values specific for the particular job.
# The job-specific system properties are passed to each spawned DQC runtime JVM associated with the identified launch type (LOCAL, SPARK, ...).
#plugin.executor-launch-model.ataccama.one.launch-type-properties.SPARK.job-specific-system-properties.allowed-keys=\
#  executorProp.spark.ata.readOnlyLatestPartition,\
#  executorProp.spark.ata.enableCustomSqlTemplates,\
#  executorProp.spark.app.name,\
#  executorProp.spark.rdd.compress,\
#  executorProp.spark.submit.deployMode,\
#  executorProp.spark.driver.cores,\
#  executorProp.spark.driver.maxResultSize,\
#  executorProp.spark.driver.memory,\
#  executorProp.spark.driver.memoryOverhead,\
#  executorProp.spark.driver.extraClassPath,\
#  executorProp.spark.driver.extraJavaOptions,\
#  executorProp.spark.driver.userClassPathFirst,\
#  executorProp.spark.executor.cores,\
#  executorProp.spark.executor.memory,\
#  executorProp.spark.executor.memoryOverhead,\
#  executorProp.spark.executor.extraClassPath,\
#  executorProp.spark.executor.extraJavaOptions,\
#  executorProp.spark.executor.userClassPathFirst,\
#  executorProp.spark.dynamicAllocation.enabled,\
#  executorProp.spark.dynamicAllocation.maxExecutors,\
#  executorProp.spark.dynamicAllocation.minExecutors


#------------------------------------- LIBRARIES CLUSTER ---------------------------------------------------------

# Classpath containing all required Java libraries. It must contain the <DQC_HOME>/lib/* folder.
# The files in the classpath are copied to the cluster.
# You can set additional classpaths in a separate cp.<custom> property,
# e.g., cp.drivers for database drivers.

# To exclude specific files from the classpath, you can use the !<file_mask> notation following your classpath:

# Relative to temp/jobs/${jobId} from root dir of the repository
plugin.executor-launch-model.ataccama.one.launch-type-properties.SPARK.cpdelim=;
plugin.executor-launch-model.ataccama.one.launch-type-properties.SPARK.cp.runtime=../../../lib/runtime/*;../../../lib/jdbc/*;../../../lib/jdbc_ext/*
plugin.executor-launch-model.ataccama.one.launch-type-properties.SPARK.cp.ovr=../../../lib/ovr/*.jar
plugin.executor-launch-model.ataccama.one.launch-type-properties.SPARK.cp.!exclude=!atc-hive-jdbc*!hive-jdbc*;!slf4j-api-*.jar;!kryo-*.jar;!scala*.jar;!commons-lang3*.jar!cif-dtdb*.jar


#------------------------------------------- AUTHENTICATION --------------------------------------------------

# Cluster authentication settings. Can be either basic or kerberos.
#plugin.executor-launch-model.ataccama.one.launch-type-properties.SPARK.auth=basic
plugin.executor-launch-model.ataccama.one.launch-type-properties.SPARK.auth=kerberos

# For a basic authentication, provide the following settings:
# basic.user - the user to be used for processing on cluster.
#plugin.executor-launch-model.ataccama.one.launch-type-properties.SPARK.basic.user=bde.user

# If the cluster is secured by Kerberos, provide the additional settings:
# Kerberos configuration, typically called krb5.conf
# Kerberos principal to be used for authentication by this BDE.
# Keytab for the principal above. To use a Kerberos ticket instead of the keytab, comment out the kerberos.keytab property.
plugin.executor-launch-model.ataccama.one.launch-type-properties.SPARK.kerberos.conf=/etc/krb5.conf
plugin.executor-launch-model.ataccama.one.launch-type-properties.SPARK.kerberos.principal=executor@EXAMPLE.COM
plugin.executor-launch-model.ataccama.one.launch-type-properties.SPARK.kerberos.keytab=../../../bin/hadoop/keytab/executor.keytab

# Impersonation property allows Executor to start processes on cluster via another user. If none specified, default true
plugin.executor-launch-model.ataccama.one.launch-type-properties.SPARK.impersonate=false












#============================================EXPERT SETTINGS=========================================================
#------------------------------------ TEMPORARY FILES LOCATION ------------------------------------------------
# Default folders for distributed local files:
# on HDFS used for user files
plugin.executor-launch-model.ataccama.one.launch-type-properties.SPARK.fsc.shared=/tmp/ataccama_executor_shared_fs/
# on HDFS used for distributing classpath (cp)
plugin.executor-launch-model.ataccama.one.launch-type-properties.SPARK.fsc.shrcp=/tmp/ataccama_executor_shared_cp/
# on local FS of Hadoop worker nodes
plugin.executor-launch-model.ataccama.one.launch-type-properties.SPARK.fsc.node=/tmp/ataccama_executor_node/
# Forces to copy files ignoring last modification time
#plugin.executor-launch-model.ataccama.one.launch-type-properties.SPARK.fsc.force=true

# Default permissions for HDFS
#plugin.executor-launch-model.ataccama.one.launch-type-properties.SPARK.conf.fs.permissions.umask-mode=022

# -------------------------------- SPARK CONFIGURATION ----------------------------------------------------------
# All properties starting with spark. are passed to Spark.
# The required properties are:
# spark.master - must be set to yarn; other modes are not supported
# spark.io.compression.codec - must be set to lz4
#
# You can set additional spark.<custom> properties here.

#----------------- ATACCAMA CONFIGURATION ----------------------------
plugin.executor-launch-model.ataccama.one.launch-type-properties.SPARK.spark.master=yarn
plugin.executor-launch-model.ataccama.one.launch-type-properties.SPARK.spark.submit.deployMode=client

plugin.executor-launch-model.ataccama.one.launch-type-properties.SPARK.spark.sql.catalogImplementation=hive
#plugin.executor-launch-model.ataccama.one.launch-type-properties.SPARK.spark.sql.hive.metastore.version=1.2.1
#plugin.executor-launch-model.ataccama.one.launch-type-properties.SPARK.spark.sql.hive.metastore.jars=builtin

# Hive libraries for Cloudera
#plugin.executor-launch-model.ataccama.one.launch-type-properties.SPARK.spark.sql.hive.metastore.jars=/opt/cloudera/parcels/CDH/lib/hadoop/client/*:/opt/cloudera/parcels/CDH/lib/hive/lib/*

# Hive libraries for Hortonworks
#plugin.executor-launch-model.ataccama.one.launch-type-properties.SPARK.spark.sql.hive.metastore.jars=/usr/hdp/current/spark2-client/jars/*

#plugin.executor-launch-model.ataccama.one.launch-type-properties.SPARK.spark.yarn.historyServer.address=http://spark-history-server.com:18081

plugin.executor-launch-model.ataccama.one.launch-type-properties.SPARK.spark.eventLog.enabled=true
# Default location for Cloudera
#plugin.executor-launch-model.ataccama.one.launch-type-properties.SPARK.spark.eventLog.dir=hdfs:///user/spark/spark2ApplicationHistory
# Default location for Hortonworks
#plugin.executor-launch-model.ataccama.one.launch-type-properties.SPARK.spark.eventLog.dir=hdfs:///spark2-history

plugin.executor-launch-model.ataccama.one.launch-type-properties.SPARK.spark.io.compression.codec=lz4

#---------------- Additional properties ----------------------------------



# Max Attempts may be used for debugging.
#plugin.executor-launch-model.ataccama.one.launch-type-properties.SPARK.spark.yarn.maxAppAttempts=1
#plugin.executor-launch-model.ataccama.one.launch-type-properties.SPARK.spark.yarn.queue=default


#------------------ Performance Optimization ----------------------------
plugin.executor-launch-model.ataccama.one.launch-type-properties.SPARK.spark.shuffle.service.enabled=true
plugin.executor-launch-model.ataccama.one.launch-type-properties.SPARK.spark.dynamicAllocation.enabled=true
plugin.executor-launch-model.ataccama.one.launch-type-properties.SPARK.spark.dynamicAllocation.maxExecutors=100
plugin.executor-launch-model.ataccama.one.launch-type-properties.SPARK.spark.dynamicAllocation.minExecutors=2

#plugin.executor-launch-model.ataccama.one.launch-type-properties.SPARK.spark.executor.instances=4

plugin.executor-launch-model.ataccama.one.launch-type-properties.SPARK.spark.executor.memory=4g
plugin.executor-launch-model.ataccama.one.launch-type-properties.SPARK.spark.kryoserializer.buffer.max=2047m

plugin.executor-launch-model.ataccama.one.launch-type-properties.SPARK.spark.executor.cores=1

#------------------------------------- MISCELLANEOUS ------------------------------------------------------------
# debug mode. Shows driver and executor classloaders in spark mode
#plugin.executor-launch-model.ataccama.one.launch-type-properties.SPARK.debug=true


# Path to the folder containing all Hadoop client configuration files (e.g., core-site.xml, hdfs-site.xml) for the given cluster.
# Not needed by default. This configuration is taken automatically from $MYDIR/client_conf/ directory
#plugin.executor-launch-model.ataccama.one.launch-type-properties.SPARK.conf=/etc/hadoop/conf/

# BDE will attempt to find SPARK_HOME in default locations. If none found, please specify a path to SPARK_HOME manually
#plugin.executor-launch-model.ataccama.one.launch-type-properties.SPARK.env.SPARK_HOME=<path_to_spark_home>
#plugin.executor-launch-model.ataccama.one.launch-type-properties.SPARK.env.SPARK_HOME=/opt/cloudera/parcels/SPARK2/lib/spark2/
#plugin.executor-launch-model.ataccama.one.launch-type-properties.SPARK.env.SPARK_HOME=/usr/hdp/current/spark2-client/

# Needed for spark2 distribution without hadoop libs
# Mostly required by CDH
#plugin.executor-launch-model.ataccama.one.launch-type-properties.SPARK.env.ENABLE_SPARK_DIST_CLASSPATH=true

# Executor will attempt to validate spark properties file
# It is possible to turn off the validation with the following option:
#plugin.executor-launch-model.ataccama.one.launch-type-properties.SPARK.env.DISABLE_VALIDATE_SPARK_PROPERTIES=true

# Enable debug level of logging in log4j
#plugin.executor-launch-model.ataccama.one.launch-type-properties.SPARK.env.ENABLE_DEBUG_LOGGING=true

# Provide client configuration to Spark job via Spark variables
#plugin.executor-launch-model.ataccama.one.launch-type-properties.SPARK.env.ENABLE_SPARK_CONF_DIR=true

# Name of spark-submit script
#plugin.executor-launch-model.ataccama.one.launch-type-properties.SPARK.env.SPARK_SUBMIT=spark-submit

# Disables timeline service
plugin.executor-launch-model.ataccama.one.launch-type-properties.SPARK.conf.yarn.timeline-service.enabled=false